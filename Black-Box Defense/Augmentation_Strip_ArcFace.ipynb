{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Strip Augmentation Run\n",
    "\n",
    "### Note:\n",
    "**Similar Protocol Applies to Patch Augmentation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Strip-5:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KerasTensor(type_spec=TensorSpec(shape=(None, 300), dtype=tf.float32, name=None), name='arc_face/Softmax:0', description=\"created by layer 'arc_face'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 300), dtype=tf.float32, name=None), name='arc_face_1/Softmax:0', description=\"created by layer 'arc_face_1'\")\n",
      "Found 5076 images belonging to 300 classes.\n",
      "Found 1621 images belonging to 300 classes.\n",
      "Epoch 1/100\n",
      "159/159 [==============================] - 65s 377ms/step - loss: 33.9926 - accuracy: 0.0000e+00 - val_loss: 31.3079 - val_accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "159/159 [==============================] - 65s 407ms/step - loss: 29.7595 - accuracy: 0.0000e+00 - val_loss: 26.1466 - val_accuracy: 0.0259 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "159/159 [==============================] - 61s 380ms/step - loss: 25.2147 - accuracy: 0.0030 - val_loss: 17.9659 - val_accuracy: 0.1518 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "159/159 [==============================] - 60s 376ms/step - loss: 20.3566 - accuracy: 0.0203 - val_loss: 13.5663 - val_accuracy: 0.3448 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "159/159 [==============================] - 60s 378ms/step - loss: 15.8003 - accuracy: 0.0814 - val_loss: 9.2578 - val_accuracy: 0.5601 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "159/159 [==============================] - 60s 376ms/step - loss: 11.8015 - accuracy: 0.2082 - val_loss: 5.7297 - val_accuracy: 0.7057 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "159/159 [==============================] - 60s 377ms/step - loss: 8.2180 - accuracy: 0.3792 - val_loss: 4.8806 - val_accuracy: 0.7859 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "159/159 [==============================] - 60s 377ms/step - loss: 5.7487 - accuracy: 0.5262 - val_loss: 2.6418 - val_accuracy: 0.8883 - lr: 0.0010\n",
      "Epoch 9/100\n",
      "159/159 [==============================] - 60s 374ms/step - loss: 3.8224 - accuracy: 0.6395 - val_loss: 2.1642 - val_accuracy: 0.9118 - lr: 0.0010\n",
      "Epoch 10/100\n",
      "159/159 [==============================] - 61s 381ms/step - loss: 2.7821 - accuracy: 0.7252 - val_loss: 1.0365 - val_accuracy: 0.9624 - lr: 0.0010\n",
      "Epoch 11/100\n",
      "159/159 [==============================] - 60s 375ms/step - loss: 2.1103 - accuracy: 0.7734 - val_loss: 0.9707 - val_accuracy: 0.9611 - lr: 0.0010\n",
      "Epoch 12/100\n",
      "159/159 [==============================] - 60s 375ms/step - loss: 1.7326 - accuracy: 0.8081 - val_loss: 0.8635 - val_accuracy: 0.9661 - lr: 0.0010\n",
      "Epoch 13/100\n",
      "159/159 [==============================] - 60s 374ms/step - loss: 1.6116 - accuracy: 0.8235 - val_loss: 0.4214 - val_accuracy: 0.9840 - lr: 0.0010\n",
      "Epoch 14/100\n",
      "159/159 [==============================] - 52s 323ms/step - loss: 1.2388 - accuracy: 0.8522 - val_loss: 0.7697 - val_accuracy: 0.9729 - lr: 0.0010\n",
      "Epoch 15/100\n",
      "159/159 [==============================] - 51s 323ms/step - loss: 1.0453 - accuracy: 0.8710 - val_loss: 0.6478 - val_accuracy: 0.9784 - lr: 0.0010\n",
      "Epoch 16/100\n",
      "159/159 [==============================] - 62s 393ms/step - loss: 0.9809 - accuracy: 0.8812 - val_loss: 0.1596 - val_accuracy: 0.9932 - lr: 0.0010\n",
      "Epoch 17/100\n",
      "159/159 [==============================] - 53s 332ms/step - loss: 0.9012 - accuracy: 0.9005 - val_loss: 0.2666 - val_accuracy: 0.9914 - lr: 0.0010\n",
      "Epoch 18/100\n",
      "159/159 [==============================] - 52s 329ms/step - loss: 1.0048 - accuracy: 0.8853 - val_loss: 0.2775 - val_accuracy: 0.9907 - lr: 0.0010\n",
      "Epoch 19/100\n",
      "159/159 [==============================] - 52s 328ms/step - loss: 1.0398 - accuracy: 0.8814 - val_loss: 0.6041 - val_accuracy: 0.9735 - lr: 0.0010\n",
      "Epoch 20/100\n",
      "159/159 [==============================] - 52s 325ms/step - loss: 0.9533 - accuracy: 0.8948 - val_loss: 0.4429 - val_accuracy: 0.9833 - lr: 0.0010\n",
      "Epoch 21/100\n",
      "159/159 [==============================] - 52s 326ms/step - loss: 0.9293 - accuracy: 0.8972 - val_loss: 0.6604 - val_accuracy: 0.9796 - lr: 0.0010\n",
      "Epoch 22/100\n",
      "159/159 [==============================] - 61s 381ms/step - loss: 0.6076 - accuracy: 0.9196 - val_loss: 0.0390 - val_accuracy: 0.9994 - lr: 2.0000e-04\n",
      "Epoch 23/100\n",
      "159/159 [==============================] - 60s 376ms/step - loss: 0.3215 - accuracy: 0.9509 - val_loss: 0.0358 - val_accuracy: 0.9994 - lr: 2.0000e-04\n",
      "Epoch 24/100\n",
      "159/159 [==============================] - 60s 373ms/step - loss: 0.2905 - accuracy: 0.9525 - val_loss: 0.0294 - val_accuracy: 0.9994 - lr: 2.0000e-04\n",
      "Epoch 25/100\n",
      "159/159 [==============================] - 52s 322ms/step - loss: 0.2928 - accuracy: 0.9553 - val_loss: 0.0383 - val_accuracy: 0.9994 - lr: 2.0000e-04\n",
      "Epoch 26/100\n",
      "159/159 [==============================] - 51s 323ms/step - loss: 0.2933 - accuracy: 0.9565 - val_loss: 0.0408 - val_accuracy: 0.9994 - lr: 2.0000e-04\n",
      "Epoch 27/100\n",
      "159/159 [==============================] - 59s 374ms/step - loss: 0.2689 - accuracy: 0.9580 - val_loss: 0.0235 - val_accuracy: 0.9994 - lr: 2.0000e-04\n",
      "Epoch 28/100\n",
      "159/159 [==============================] - 51s 322ms/step - loss: 0.2523 - accuracy: 0.9604 - val_loss: 0.0330 - val_accuracy: 0.9994 - lr: 2.0000e-04\n",
      "Epoch 29/100\n",
      "159/159 [==============================] - 51s 323ms/step - loss: 0.2642 - accuracy: 0.9582 - val_loss: 0.0377 - val_accuracy: 0.9994 - lr: 2.0000e-04\n",
      "Epoch 30/100\n",
      "159/159 [==============================] - 59s 372ms/step - loss: 0.2228 - accuracy: 0.9661 - val_loss: 0.0227 - val_accuracy: 0.9994 - lr: 2.0000e-04\n",
      "Epoch 31/100\n",
      "159/159 [==============================] - 59s 372ms/step - loss: 0.2089 - accuracy: 0.9675 - val_loss: 0.0205 - val_accuracy: 0.9994 - lr: 2.0000e-04\n",
      "Epoch 32/100\n",
      "159/159 [==============================] - 51s 320ms/step - loss: 0.2281 - accuracy: 0.9671 - val_loss: 0.0409 - val_accuracy: 0.9994 - lr: 2.0000e-04\n",
      "Epoch 33/100\n",
      "159/159 [==============================] - 51s 323ms/step - loss: 0.2140 - accuracy: 0.9669 - val_loss: 0.0713 - val_accuracy: 0.9981 - lr: 2.0000e-04\n",
      "Epoch 34/100\n",
      "159/159 [==============================] - 52s 324ms/step - loss: 0.2189 - accuracy: 0.9673 - val_loss: 0.0279 - val_accuracy: 0.9994 - lr: 2.0000e-04\n",
      "Epoch 35/100\n",
      "159/159 [==============================] - 52s 324ms/step - loss: 0.2042 - accuracy: 0.9689 - val_loss: 0.0341 - val_accuracy: 0.9994 - lr: 2.0000e-04\n",
      "Epoch 36/100\n",
      "159/159 [==============================] - 61s 381ms/step - loss: 0.2276 - accuracy: 0.9679 - val_loss: 0.0075 - val_accuracy: 0.9994 - lr: 2.0000e-04\n",
      "Epoch 37/100\n",
      "159/159 [==============================] - 60s 374ms/step - loss: 0.2112 - accuracy: 0.9706 - val_loss: 0.0070 - val_accuracy: 0.9994 - lr: 2.0000e-04\n",
      "Epoch 38/100\n",
      "159/159 [==============================] - 51s 321ms/step - loss: 0.2070 - accuracy: 0.9687 - val_loss: 0.0285 - val_accuracy: 0.9994 - lr: 2.0000e-04\n",
      "Epoch 39/100\n",
      "159/159 [==============================] - 51s 322ms/step - loss: 0.2050 - accuracy: 0.9687 - val_loss: 0.0277 - val_accuracy: 0.9994 - lr: 2.0000e-04\n",
      "Epoch 40/100\n",
      "159/159 [==============================] - 59s 374ms/step - loss: 0.1750 - accuracy: 0.9736 - val_loss: 0.0041 - val_accuracy: 0.9994 - lr: 2.0000e-04\n",
      "Epoch 41/100\n",
      "159/159 [==============================] - 51s 321ms/step - loss: 0.1855 - accuracy: 0.9699 - val_loss: 0.0246 - val_accuracy: 0.9981 - lr: 2.0000e-04\n",
      "Epoch 42/100\n",
      "159/159 [==============================] - 51s 322ms/step - loss: 0.1977 - accuracy: 0.9738 - val_loss: 0.0453 - val_accuracy: 0.9988 - lr: 2.0000e-04\n",
      "Epoch 43/100\n",
      "159/159 [==============================] - 51s 322ms/step - loss: 0.1698 - accuracy: 0.9740 - val_loss: 0.0242 - val_accuracy: 0.9994 - lr: 2.0000e-04\n",
      "Epoch 44/100\n",
      "159/159 [==============================] - 51s 321ms/step - loss: 0.1888 - accuracy: 0.9724 - val_loss: 0.0257 - val_accuracy: 0.9994 - lr: 2.0000e-04\n",
      "Epoch 45/100\n",
      "159/159 [==============================] - 51s 322ms/step - loss: 0.2158 - accuracy: 0.9685 - val_loss: 0.0271 - val_accuracy: 0.9994 - lr: 2.0000e-04\n",
      "Epoch 46/100\n",
      "159/159 [==============================] - 52s 327ms/step - loss: 0.2112 - accuracy: 0.9673 - val_loss: 0.0254 - val_accuracy: 0.9994 - lr: 4.0000e-05\n",
      "Epoch 47/100\n",
      "159/159 [==============================] - 59s 373ms/step - loss: 0.1951 - accuracy: 0.9720 - val_loss: 3.0490e-05 - val_accuracy: 1.0000 - lr: 4.0000e-05\n",
      "Epoch 48/100\n",
      "159/159 [==============================] - 60s 374ms/step - loss: 0.1642 - accuracy: 0.9752 - val_loss: 1.3290e-05 - val_accuracy: 1.0000 - lr: 4.0000e-05\n",
      "Epoch 49/100\n",
      "159/159 [==============================] - 60s 373ms/step - loss: 0.1648 - accuracy: 0.9768 - val_loss: 3.2295e-06 - val_accuracy: 1.0000 - lr: 4.0000e-05\n",
      "Epoch 50/100\n",
      "159/159 [==============================] - 51s 322ms/step - loss: 0.1760 - accuracy: 0.9728 - val_loss: 5.1160e-06 - val_accuracy: 1.0000 - lr: 4.0000e-05\n",
      "Epoch 51/100\n",
      "159/159 [==============================] - 51s 323ms/step - loss: 0.1752 - accuracy: 0.9732 - val_loss: 4.0137e-06 - val_accuracy: 1.0000 - lr: 4.0000e-05\n",
      "Epoch 52/100\n",
      "159/159 [==============================] - 59s 374ms/step - loss: 0.1518 - accuracy: 0.9781 - val_loss: 2.1764e-06 - val_accuracy: 1.0000 - lr: 4.0000e-05\n",
      "Epoch 53/100\n",
      "159/159 [==============================] - 60s 375ms/step - loss: 0.1563 - accuracy: 0.9754 - val_loss: 1.9945e-06 - val_accuracy: 1.0000 - lr: 1.0000e-05\n",
      "Epoch 54/100\n",
      "159/159 [==============================] - 60s 373ms/step - loss: 0.1440 - accuracy: 0.9771 - val_loss: 1.5127e-06 - val_accuracy: 1.0000 - lr: 1.0000e-05\n",
      "Epoch 55/100\n",
      "159/159 [==============================] - 60s 372ms/step - loss: 0.1468 - accuracy: 0.9775 - val_loss: 1.3810e-06 - val_accuracy: 1.0000 - lr: 1.0000e-05\n",
      "Epoch 56/100\n",
      "159/159 [==============================] - 59s 371ms/step - loss: 0.1552 - accuracy: 0.9771 - val_loss: 1.0867e-06 - val_accuracy: 1.0000 - lr: 1.0000e-05\n",
      "Epoch 57/100\n",
      "159/159 [==============================] - 59s 372ms/step - loss: 0.1607 - accuracy: 0.9779 - val_loss: 1.0623e-06 - val_accuracy: 1.0000 - lr: 1.0000e-05\n",
      "Epoch 58/100\n",
      "159/159 [==============================] - 51s 321ms/step - loss: 0.1609 - accuracy: 0.9746 - val_loss: 1.1263e-06 - val_accuracy: 1.0000 - lr: 1.0000e-05\n",
      "Epoch 59/100\n",
      "159/159 [==============================] - 59s 374ms/step - loss: 0.1660 - accuracy: 0.9754 - val_loss: 1.0189e-06 - val_accuracy: 1.0000 - lr: 1.0000e-05\n",
      "Epoch 60/100\n",
      "159/159 [==============================] - 59s 371ms/step - loss: 0.1588 - accuracy: 0.9740 - val_loss: 8.8950e-07 - val_accuracy: 1.0000 - lr: 1.0000e-05\n",
      "Epoch 61/100\n",
      "159/159 [==============================] - 51s 319ms/step - loss: 0.1573 - accuracy: 0.9777 - val_loss: 1.1454e-06 - val_accuracy: 1.0000 - lr: 1.0000e-05\n",
      "Epoch 62/100\n",
      "159/159 [==============================] - 59s 374ms/step - loss: 0.1478 - accuracy: 0.9787 - val_loss: 4.6115e-07 - val_accuracy: 1.0000 - lr: 1.0000e-05\n",
      "Epoch 63/100\n",
      "159/159 [==============================] - 51s 319ms/step - loss: 0.1698 - accuracy: 0.9752 - val_loss: 5.2442e-07 - val_accuracy: 1.0000 - lr: 1.0000e-05\n",
      "Epoch 64/100\n",
      "159/159 [==============================] - 59s 371ms/step - loss: 0.1415 - accuracy: 0.9789 - val_loss: 3.6164e-07 - val_accuracy: 1.0000 - lr: 1.0000e-05\n",
      "Epoch 65/100\n",
      "159/159 [==============================] - 60s 373ms/step - loss: 0.1643 - accuracy: 0.9760 - val_loss: 2.4837e-07 - val_accuracy: 1.0000 - lr: 1.0000e-05\n",
      "Epoch 66/100\n",
      "159/159 [==============================] - 60s 374ms/step - loss: 0.1556 - accuracy: 0.9760 - val_loss: 2.1669e-07 - val_accuracy: 1.0000 - lr: 1.0000e-05\n",
      "Epoch 67/100\n",
      "159/159 [==============================] - 51s 320ms/step - loss: 0.1499 - accuracy: 0.9773 - val_loss: 2.2911e-07 - val_accuracy: 1.0000 - lr: 1.0000e-05\n",
      "Epoch 68/100\n",
      "159/159 [==============================] - 59s 372ms/step - loss: 0.1512 - accuracy: 0.9760 - val_loss: 9.4492e-08 - val_accuracy: 1.0000 - lr: 1.0000e-05\n",
      "Epoch 69/100\n",
      "159/159 [==============================] - 51s 320ms/step - loss: 0.1552 - accuracy: 0.9783 - val_loss: 9.4860e-08 - val_accuracy: 1.0000 - lr: 1.0000e-05\n",
      "Epoch 70/100\n",
      "159/159 [==============================] - 59s 372ms/step - loss: 0.1614 - accuracy: 0.9726 - val_loss: 2.9048e-08 - val_accuracy: 1.0000 - lr: 1.0000e-05\n",
      "Epoch 71/100\n",
      "159/159 [==============================] - 51s 319ms/step - loss: 0.1577 - accuracy: 0.9722 - val_loss: 4.5300e-08 - val_accuracy: 1.0000 - lr: 1.0000e-05\n",
      "Epoch 72/100\n",
      "159/159 [==============================] - 51s 321ms/step - loss: 0.1584 - accuracy: 0.9752 - val_loss: 3.2725e-08 - val_accuracy: 1.0000 - lr: 1.0000e-05\n",
      "Epoch 73/100\n",
      "159/159 [==============================] - 51s 321ms/step - loss: 0.1523 - accuracy: 0.9785 - val_loss: 3.4710e-08 - val_accuracy: 1.0000 - lr: 1.0000e-05\n",
      "Epoch 74/100\n",
      "159/159 [==============================] - 60s 375ms/step - loss: 0.1472 - accuracy: 0.9768 - val_loss: 2.0591e-08 - val_accuracy: 1.0000 - lr: 1.0000e-05\n",
      "Epoch 75/100\n",
      "159/159 [==============================] - 59s 372ms/step - loss: 0.1550 - accuracy: 0.9748 - val_loss: 1.7723e-08 - val_accuracy: 1.0000 - lr: 1.0000e-05\n",
      "Epoch 76/100\n",
      "159/159 [==============================] - 51s 321ms/step - loss: 0.1580 - accuracy: 0.9762 - val_loss: 1.8311e-08 - val_accuracy: 1.0000 - lr: 1.0000e-05\n",
      "Epoch 77/100\n",
      "159/159 [==============================] - 59s 373ms/step - loss: 0.1314 - accuracy: 0.9825 - val_loss: 1.4267e-08 - val_accuracy: 1.0000 - lr: 1.0000e-05\n",
      "Epoch 78/100\n",
      "159/159 [==============================] - 51s 320ms/step - loss: 0.1472 - accuracy: 0.9750 - val_loss: 1.5738e-08 - val_accuracy: 1.0000 - lr: 1.0000e-05\n",
      "Epoch 79/100\n",
      "159/159 [==============================] - 59s 373ms/step - loss: 0.1425 - accuracy: 0.9781 - val_loss: 9.9279e-09 - val_accuracy: 1.0000 - lr: 1.0000e-05\n",
      "Epoch 80/100\n",
      "159/159 [==============================] - 60s 374ms/step - loss: 0.1522 - accuracy: 0.9748 - val_loss: 7.3540e-09 - val_accuracy: 1.0000 - lr: 1.0000e-05\n",
      "Epoch 81/100\n",
      "159/159 [==============================] - 59s 371ms/step - loss: 0.1532 - accuracy: 0.9783 - val_loss: 3.4564e-09 - val_accuracy: 1.0000 - lr: 1.0000e-05\n",
      "Epoch 82/100\n",
      "159/159 [==============================] - 59s 372ms/step - loss: 0.1377 - accuracy: 0.9783 - val_loss: 2.8681e-09 - val_accuracy: 1.0000 - lr: 1.0000e-05\n",
      "Epoch 83/100\n",
      "159/159 [==============================] - 51s 319ms/step - loss: 0.1534 - accuracy: 0.9777 - val_loss: 5.9568e-09 - val_accuracy: 1.0000 - lr: 1.0000e-05\n",
      "Epoch 84/100\n",
      "159/159 [==============================] - 51s 320ms/step - loss: 0.1316 - accuracy: 0.9823 - val_loss: 3.0152e-09 - val_accuracy: 1.0000 - lr: 1.0000e-05\n",
      "Epoch 85/100\n",
      "159/159 [==============================] - 60s 375ms/step - loss: 0.1588 - accuracy: 0.9714 - val_loss: 2.2798e-09 - val_accuracy: 1.0000 - lr: 1.0000e-05\n",
      "Epoch 86/100\n",
      "159/159 [==============================] - 51s 319ms/step - loss: 0.1426 - accuracy: 0.9777 - val_loss: 4.0447e-09 - val_accuracy: 1.0000 - lr: 1.0000e-05\n",
      "Epoch 87/100\n",
      "159/159 [==============================] - 51s 321ms/step - loss: 0.1484 - accuracy: 0.9787 - val_loss: 4.3389e-09 - val_accuracy: 1.0000 - lr: 1.0000e-05\n",
      "Epoch 88/100\n",
      "159/159 [==============================] - 51s 321ms/step - loss: 0.1402 - accuracy: 0.9775 - val_loss: 3.0152e-09 - val_accuracy: 1.0000 - lr: 1.0000e-05\n",
      "Epoch 89/100\n",
      "159/159 [==============================] - 59s 373ms/step - loss: 0.1484 - accuracy: 0.9770 - val_loss: 1.1031e-09 - val_accuracy: 1.0000 - lr: 1.0000e-05\n",
      "Epoch 90/100\n",
      "159/159 [==============================] - 51s 321ms/step - loss: 0.1477 - accuracy: 0.9764 - val_loss: 1.5444e-09 - val_accuracy: 1.0000 - lr: 1.0000e-05\n",
      "Epoch 91/100\n",
      "159/159 [==============================] - 51s 320ms/step - loss: 0.1536 - accuracy: 0.9789 - val_loss: 1.1766e-09 - val_accuracy: 1.0000 - lr: 1.0000e-05\n",
      "Epoch 92/100\n",
      "159/159 [==============================] - 59s 372ms/step - loss: 0.1516 - accuracy: 0.9752 - val_loss: 7.3541e-10 - val_accuracy: 1.0000 - lr: 1.0000e-05\n",
      "Epoch 93/100\n",
      "159/159 [==============================] - 59s 371ms/step - loss: 0.1421 - accuracy: 0.9773 - val_loss: 5.8832e-10 - val_accuracy: 1.0000 - lr: 1.0000e-05\n",
      "Epoch 94/100\n",
      "159/159 [==============================] - 59s 371ms/step - loss: 0.1385 - accuracy: 0.9807 - val_loss: 4.4124e-10 - val_accuracy: 1.0000 - lr: 1.0000e-05\n",
      "Epoch 95/100\n",
      "159/159 [==============================] - 59s 372ms/step - loss: 0.1376 - accuracy: 0.9789 - val_loss: 2.9416e-10 - val_accuracy: 1.0000 - lr: 1.0000e-05\n",
      "Epoch 96/100\n",
      "159/159 [==============================] - 51s 318ms/step - loss: 0.1467 - accuracy: 0.9783 - val_loss: 5.1478e-10 - val_accuracy: 1.0000 - lr: 1.0000e-05\n",
      "Epoch 97/100\n",
      "159/159 [==============================] - 59s 372ms/step - loss: 0.1360 - accuracy: 0.9791 - val_loss: 2.2062e-10 - val_accuracy: 1.0000 - lr: 1.0000e-05\n",
      "Epoch 98/100\n",
      "159/159 [==============================] - 51s 320ms/step - loss: 0.1595 - accuracy: 0.9748 - val_loss: 3.6770e-10 - val_accuracy: 1.0000 - lr: 1.0000e-05\n",
      "Epoch 99/100\n",
      "159/159 [==============================] - 51s 320ms/step - loss: 0.1390 - accuracy: 0.9795 - val_loss: 7.4276e-09 - val_accuracy: 1.0000 - lr: 1.0000e-05\n",
      "Epoch 100/100\n",
      "159/159 [==============================] - 51s 321ms/step - loss: 0.1390 - accuracy: 0.9785 - val_loss: 5.2949e-09 - val_accuracy: 1.0000 - lr: 1.0000e-05\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x20a57e14f40>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# Set a seed for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "def remove_strip(image, orientation='horizontal', strip_size=5):\n",
    "    if orientation == 'horizontal':\n",
    "        # Choose a random horizontal strip to remove\n",
    "        y = np.random.randint(0, image.shape[0] - strip_size)\n",
    "        image[y:y+strip_size, :, :] = 0\n",
    "    else:\n",
    "        # Choose a random vertical strip to remove\n",
    "        x = np.random.randint(0, image.shape[1] - strip_size)\n",
    "        image[:, x:x+strip_size, :] = 0\n",
    "    return image\n",
    "\n",
    "def strip_removal(image):  # Only takes one argument\n",
    "    rand_num = np.random.rand()\n",
    "    if rand_num < 0.15:\n",
    "        orientation = 'horizontal'\n",
    "        image = remove_strip(image, orientation)\n",
    "    elif rand_num < 0.3:\n",
    "        orientation = 'vertical'\n",
    "        image = remove_strip(image, orientation)\n",
    "    return image  # Return the image\n",
    "\n",
    "# Load the pre-trained Xception model\n",
    "base_model = tf.keras.applications.Xception(input_shape=(256, 256, 3),\n",
    "                                            include_top=False,\n",
    "                                            weights='imagenet')\n",
    "\n",
    "# Unfreeze the base model\n",
    "base_model.trainable = True\n",
    "\n",
    "# Fine-tune from this layer onwards\n",
    "fine_tune_at = 100\n",
    "\n",
    "# Freeze all the layers before the `fine_tune_at` layer\n",
    "for layer in base_model.layers[:fine_tune_at]:\n",
    "  layer.trainable =  False\n",
    "\n",
    "# Defining my ArcFace Function\n",
    "class ArcFace(layers.Layer):\n",
    "    def __init__(self, n_classes=300, s=60.0, m=0.5, regularizer=None, **kwargs):\n",
    "        super(ArcFace, self).__init__(**kwargs)\n",
    "        self.n_classes = n_classes\n",
    "        self.s = s\n",
    "        self.m = m\n",
    "        self.regularizer = regularizer\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        super(ArcFace, self).build(input_shape[0])\n",
    "        self.W = self.add_weight(name='W',\n",
    "                                 shape=(input_shape[0][-1], self.n_classes),\n",
    "                                 initializer='glorot_uniform',\n",
    "                                 trainable=True,\n",
    "                                 regularizer=self.regularizer)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x, y = inputs\n",
    "        c = tf.shape(x)[-1]\n",
    "        # normalize feature\n",
    "        x = tf.nn.l2_normalize(x, axis=1)\n",
    "        # normalize weights\n",
    "        W = tf.nn.l2_normalize(self.W, axis=0)\n",
    "        # dot product\n",
    "        logits = x @ W\n",
    "        # add margin\n",
    "        theta = tf.acos(tf.clip_by_value(logits, -1.0 + tf.keras.backend.epsilon(), 1.0 - tf.keras.backend.epsilon()))\n",
    "        target_logits = tf.cos(theta + self.m)\n",
    "        logits = logits * (1 - y) + target_logits * y\n",
    "        # feature re-scale\n",
    "        logits *= self.s\n",
    "        out = tf.nn.softmax(logits)\n",
    "\n",
    "        return out\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config().copy()\n",
    "        config.update({\n",
    "            'n_classes': self.n_classes,\n",
    "            's': self.s,\n",
    "            'm': self.m,\n",
    "            'regularizer': self.regularizer,\n",
    "        })\n",
    "        return config\n",
    "\n",
    "# Add new layers\n",
    "x = base_model.output\n",
    "x = layers.Flatten()(x)\n",
    "x = layers.Dense(1024, activation='relu')(x)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "\n",
    "# Placeholder for label input\n",
    "y = tf.keras.Input(shape=(300,))  # assuming you have 300 classes\n",
    "output = ArcFace(n_classes=300)([x, y])\n",
    "print(output)\n",
    "\n",
    "# Placeholder for label input\n",
    "y = tf.keras.Input(shape=(300,))  # assuming you have 300 classes\n",
    "output = ArcFace(n_classes=300)([x, y])\n",
    "print(output)\n",
    "\n",
    "# Define the new model\n",
    "model = Model(inputs=[base_model.input, y], outputs=output)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(),\n",
    "              loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Add the patch removal function to your ImageDataGenerator\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=10,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    horizontal_flip=True,\n",
    "    brightness_range=[0.8, 1.2],\n",
    "    preprocessing_function=strip_removal\n",
    ")\n",
    "\n",
    "val_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# Custom data generator\n",
    "class CustomDataGen(tf.keras.utils.Sequence):\n",
    "    def __init__(self, directory, datagen, batch_size=32, target_size=(256, 256), class_mode='categorical'):\n",
    "        self.generator = datagen.flow_from_directory(directory, target_size=target_size, batch_size=batch_size, class_mode=class_mode)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.generator)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        x, y = self.generator[index]\n",
    "        return [x, y], y\n",
    "\n",
    "# Create custom data generators\n",
    "train_generator = CustomDataGen('ocular_recognition/train', train_datagen)\n",
    "val_generator = CustomDataGen('ocular_recognition/validation', val_datagen)\n",
    "\n",
    "# Set up callbacks\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=10)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.00001)\n",
    "os.makedirs('trained_models', exist_ok=True)\n",
    "model_checkpoint = ModelCheckpoint('trained_models/Xception_best_model_ArcFace_Strip5.h5', save_best_only=True)\n",
    "\n",
    "callbacks = [early_stop, reduce_lr, model_checkpoint]\n",
    "\n",
    "# Fit the model on the batches generated by datagen.flow()\n",
    "model.fit(train_generator,\n",
    "          validation_data=val_generator,\n",
    "          epochs=100,\n",
    "          callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1621 images belonging to 300 classes.\n",
      "51/51 [==============================] - 5s 91ms/step - loss: 5.1478e-09 - accuracy: 1.0000\n",
      "Test accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Create a data generator for the test data\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# Custom data generator\n",
    "class CustomDataGen(tf.keras.utils.Sequence):\n",
    "    def __init__(self, directory, datagen, batch_size=32, target_size=(256, 256), class_mode='categorical'):\n",
    "        self.generator = datagen.flow_from_directory(directory, target_size=target_size, batch_size=batch_size, class_mode=class_mode)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.generator)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        x, y = self.generator[index]\n",
    "        return [x, y], y\n",
    "\n",
    "# Load images from the test directory\n",
    "test_generator = CustomDataGen('ocular_recognition/test', test_datagen)\n",
    "\n",
    "# Evaluate the model on the test data\n",
    "test_loss, test_acc = model.evaluate(test_generator)\n",
    "print(f'Test accuracy: {test_acc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 443ms/step\n",
      "Predicted class for the single image: 1141\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Load the saved model\n",
    "# model = load_model('trained_models/Xception_best_model_ArcFace_Strip5.h5', custom_objects={'ArcFace': ArcFace})\n",
    "\n",
    "# Load and preprocess a single image for prediction\n",
    "image_path = 'ocular_recognition/test/1141/1141_l_1.png'\n",
    "img = image.load_img(image_path, target_size=(256, 256))\n",
    "img_array = image.img_to_array(img)\n",
    "img_array = np.expand_dims(img_array, axis=0)\n",
    "img_array /= 255.0  # Normalize pixel values to the range [0, 1]\n",
    "\n",
    "# Create dummy labels\n",
    "dummy_labels = np.zeros((1, 300))  # assuming you have 300 classes\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.predict([img_array, dummy_labels])\n",
    "\n",
    "# Get the class label with the highest probability\n",
    "predicted_class = np.argmax(predictions[0])\n",
    "\n",
    "# Get the class labels\n",
    "class_labels = list(test_generator.generator.class_indices.keys())\n",
    "\n",
    "# Get the predicted class label\n",
    "predicted_class_label = class_labels[predicted_class]\n",
    "\n",
    "print(f'Predicted class for the single image: {predicted_class_label}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorTorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
